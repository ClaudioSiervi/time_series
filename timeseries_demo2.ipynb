{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Power System Data\n",
    "## Timeseries data\n",
    "This is a python script that downloads and processes renewables in-feed data from the German TSOs Amprion and TransnetBW.\n",
    "The Ouput is one CSV file containing all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "* [1. Settings](#1.-Settings)\n",
    "\t* [1.1 Libraries](#1.1-Libraries)\n",
    "\t* [1.2 Folder path](#1.2-Folder-path)\n",
    "\t* [1.3 Parameters for download sources](#1.3-Parameters-for-download-sources)\n",
    "* [2. Download](#2.-Download)\n",
    "\t* [2.1 Download funtions](#2.1-Download-funtions)\n",
    "\t* [2.2 Download loop](#2.2-Download-loop)\n",
    "* [3. Process the data](#3.-Process-the-data)\n",
    "\t* [3.1 Read function](#3.1-Read-function)\n",
    "\t* [3.2 reading files one by one](#3.2-reading-files-one-by-one)\n",
    "\t* [3.3 Display the Dataset](#3.3-Display-the-Dataset)\n",
    "* [4. Save csv file to disk](#4.-Save-csv-file-to-disk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import requests\n",
    "import logging\n",
    "logger = logging.getLogger('log')\n",
    "logger.setLevel('INFO')\n",
    "import os\n",
    "from dateutil.rrule import *\n",
    "from dateutil.relativedelta import *\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Folder path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates folders inside the notebook's directory on the users computer for the downloaded data and the outputfiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "downloadpath = 'downloads1/'\n",
    "outputpath = 'output1/'\n",
    "outputfile = 'output_1.csv'\n",
    "if not os.path.exists(downloadpath): os.makedirs(downloadpath)\n",
    "if not os.path.exists(outputpath): os.makedirs(outputpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parameters for download sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a python dictionary containing input parameters needed to generate the URLs belonging to the data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = \"\"\"    \n",
    "    Amprion:\n",
    "        wind: \n",
    "            url_template: http://amprion.de/applications/applicationfiles/winddaten2.php?mode=download&format=csv&start={u_start.day}.{u_start.month}.{u_start.year}&end={u_end.day}.{u_end.month}.{u_end.year}\n",
    "            bundle: complete\n",
    "            start: 2008-01-04\n",
    "            end: recent\n",
    "            filetype: csv \n",
    "        pv: \n",
    "            url_template: http://amprion.de/applications/applicationfiles/PV_einspeisung.php?mode=download&format=csv&start={u_start.day}.{u_start.month}.{u_start.year}&end={u_end.day}.{u_end.month}.{u_end.year}\n",
    "            bundle: complete\n",
    "            start: 2010-01-07\n",
    "            end: recent\n",
    "            filetype: csv \n",
    "    TransnetBW: \n",
    "        wind: \n",
    "            url_template: https://www.transnetbw.de/de/kennzahlen/erneuerbare-energien/windenergie?app=wind&activeTab=csv&selectMonatDownload={month}&view=1&download=true\n",
    "            bundle: special\n",
    "            start: 2010-01-01\n",
    "            end: recent\n",
    "            filetype: csv       \n",
    "        pv: \n",
    "            url_template: https://www.transnetbw.de/de/kennzahlen/erneuerbare-energien/fotovoltaik?app=solar&activeTab=csv&selectMonatDownload={month}&view=1&download=true\n",
    "            bundle: special\n",
    "            start: 2011-01-01\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "\"\"\"\n",
    "conf = yaml.load(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf = \"\"\"    \n",
    "    50hertz: \n",
    "        wind: \n",
    "            url_template: http://ws.50hertz.com/web01/api/WindPowerForecast/DownloadFile?fileName={u_start:%Y}.csv&callback=?\n",
    "            bundle: YEARLY\n",
    "            start: 2005-01-01\n",
    "            end: recent\n",
    "            filetype: csv        \n",
    "        pv: \n",
    "            url_template: http://ws.50hertz.com/web01/api/PhotovoltaicForecast/DownloadFile?fileName={u_start:%Y}.csv&callback=?\n",
    "            bundle: YEARLY\n",
    "            start: 2012-01-01\n",
    "            end: recent\n",
    "            filetype: csv  \n",
    "\"\"\"\n",
    "conf = yaml.load(conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Download funtions\n",
    "In this section we define some functions that generate URLS from parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url(url_template, filetype, source, tech, start, end):\n",
    "    \"\"\"construct URLs from a template, filling in start- and enddates and call download funtion.\"\"\"    \n",
    "    filename = source+'_'+tech+'_'+start.strftime('%Y-%m-%d')+'_'+end.strftime('%Y-%m-%d')\n",
    "    full_url = url_template.format(u_start = start, u_end = end)\n",
    "    download(full_url, filename, filetype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_url_TransnetBW(url_template, filetype, count, source, tech):\n",
    "    \"\"\"construct URLs from a template, filling in counter and call download funtion.\"\"\"   \n",
    "    filename = source+'_'+tech+'_'+str(count)\n",
    "    full_url = url_template.format(month = count)\n",
    "    download(full_url, filename, filetype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the actual download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def download(full_url, filename, filetype):\n",
    "    \"\"\"download and save file from URL and log original filename.\"\"\"    \n",
    "    logger.info('Attempting download of:')\n",
    "    logger.info(filename)\n",
    "    logger.info('From URL:')    \n",
    "    logger.info(full_url)\n",
    "    full_filename = downloadpath+filename+'.'+filetype\n",
    "    if os.path.exists(full_filename):\n",
    "        logger.info('Filename already exists. Skip to next.')\n",
    "    else:\n",
    "        resp = requests.get(full_url, stream = True)\n",
    "        original_filename = resp.headers['content-disposition'].split('filename=')[-1]\n",
    "        logger.info('original_filename:')\n",
    "        logger.info(original_filename)\n",
    "        with open(full_filename, 'wb') as output_file:\n",
    "            for chunk in resp.iter_content(1024):\n",
    "                output_file.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Download loop\n",
    "Here we iterate over the sources and technology (wind/solar) entries specified above and download the data for a specified period, in this case the year 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules = {'YEARLY': YEARLY,\n",
    "         'MONTHLY': MONTHLY,\n",
    "         'DAILY': DAILY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for source, tech in conf.items():\n",
    "    for tech, param in tech.items():\n",
    "        if source == 'TransnetBW': # TransnetBW just counts the months backwards, this requires a different approach\n",
    "            for count in range(12,24): # iterate over range from 12 to 23, this are the months of 2014\n",
    "                make_url_TransnetBW(param['url_template'], param['filetype'], count, source, tech)\n",
    "        else:\n",
    "            start = param['start']\n",
    "        \n",
    "            if param['end'] == 'recent':\n",
    "                end = datetime(2014,12,31)\n",
    "            else:\n",
    "                end = param['end']\n",
    "#            start = datetime(2014,1,1)\n",
    "#            end = datetime(2014,12,31)\n",
    "\n",
    "            if param['bundle'] == 'complete':\n",
    "                make_url(param['url_template'], param['filetype'], source, tech, start, end)\n",
    "            else:\n",
    "                break_dates = rrule(rules[param['bundle']], dtstart = start, until = end)\n",
    "                for date in break_dates:\n",
    "                    p_start = date.replace(day = 1)\n",
    "                    if param['bundle'] == 'YEARLY':\n",
    "                        p_end = p_start + relativedelta(years = 1)\n",
    "                    if param['bundle'] == 'MONTHLY':\n",
    "                        p_end = p_start + relativedelta(months = 1)\n",
    "\n",
    "                    make_url(param['url_template'], param['filetype'], source, tech, p_start, p_end)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Process the data\n",
    "\n",
    "We want to merge the downloadet files into one big CSV file. Since every TSO provides the data in a different format, this requires custom read functionality for every source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Read function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readData(filePath, source, tech):\n",
    "    \"\"\"Read data from a CSV file taking into account source peculiarities\"\"\"\n",
    "    \n",
    "    if os.path.getsize(filePath) < 128:\n",
    "        print(\"file is smaller than 128 Byte, which means it's probably empty\")\n",
    "        data = pd.DataFrame() # an empty DataFrame\n",
    "        return data\n",
    "\n",
    "    elif source == 'TransnetBW':\n",
    "        data = pd.read_csv(\n",
    "            filePath,\n",
    "            sep = \";\",\n",
    "            header = 0,\n",
    "            index_col = \"dt_index\",\n",
    "            names = ['input_date', 'input_time', source+'_'+tech+'_forecast', source+'_'+tech+'_actual'],\n",
    "            parse_dates = {'dt_index' : ['input_date', 'input_time',]},\n",
    "            date_parser = None,         \n",
    "            dayfirst = True,\n",
    "            decimal = ',',\n",
    "            converters = None,\n",
    "            usecols = [0,1,4,5],\n",
    "        )\n",
    "        \n",
    "        data.index = data.index.tz_localize('Europe/Berlin', ambiguous = 'infer')\n",
    "        \n",
    "        actualCol = source+'_'+tech+'_actual'    \n",
    "        forecastCol = source+'_'+tech+'_forecast'\n",
    "        data = data[[actualCol, forecastCol]]\n",
    "        \n",
    "    elif source == 'Amprion':\n",
    "        data = pd.read_csv(\n",
    "            filePath,\n",
    "            sep = \";\",\n",
    "            header = 0,\n",
    "            index_col = 'dt_index',\n",
    "            names = ['input_date', 'input_time', source+'_'+tech+'_forecast', source+'_'+tech+'_actual'],\n",
    "            parse_dates = {'dt_index' : ['input_date', 'input_time']},\n",
    "            date_parser = None,\n",
    "            dayfirst = True,\n",
    "            decimal = ',',\n",
    "            converters = {'input_time' : lambda x: x[:6]},\n",
    "            usecols = [0,1,2,3],        \n",
    "        )\n",
    "        \n",
    "        dst_col = np.ones(len(data.index), dtype=bool)        \n",
    "        data.index = data.index.tz_localize('Europe/Berlin', ambiguous=dst_col)\n",
    "        \n",
    "        actualCol = source+'_'+tech+'_actual'    \n",
    "        forecastCol = source+'_'+tech+'_forecast'\n",
    "        data = data[[actualCol, forecastCol]]\n",
    "\n",
    "    if source == '50hertz':\n",
    "        data = pd.read_csv(\n",
    "            filePath,\n",
    "            sep = \";\",\n",
    "            header = 3,\n",
    "            index_col = 'dt_index',\n",
    "            names = ['input_date', 'input_time', source+'_'+tech+'_actual'],\n",
    "            parse_dates = {'dt_index' : ['input_date', 'input_time']},\n",
    "            date_parser = None,\n",
    "            dayfirst = True,\n",
    "            decimal = ',',\n",
    "            thousands = '.',\n",
    "            converters = {'input_time' : lambda x: x[:5]},\n",
    "            usecols = [0,1,3],\n",
    "        )\n",
    "        \n",
    "        if not 2006 > pd.to_datetime(data.index.values[0]).year > 2015: # Until 2006 as well as  in 2015, only the B-hour in October is present, the A-hour is missing in the data \n",
    "            dst_col = np.zeros(len(data.index), dtype=bool)\n",
    "            data.index = data.index.tz_localize('Europe/Berlin', ambiguous = dst_col)\n",
    "        else:\n",
    "            data.index = data.index.tz_localize('Europe/Berlin', ambiguous = 'infer')\n",
    "\n",
    "        actualCol = source+'_'+tech+'_actual'    \n",
    "        data = data[actualCol]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 reading files one by one\n",
    "For each source/TSO and technology specified in the conf dict, this section finds all the downloaded files in the downloads folder and then calls the readData function with the relevant parameters on each file.\n",
    "The datasets returned by the read function are then merged into one large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:log:reading\n",
      "INFO:log:50hertz_wind_2005-01-01_2006-01-01.csv\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Must specify axis=0 or 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-8b36121f4e61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m                     \u001b[0mdataToAdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadData\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownloadpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtech\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m                     \u001b[0mresultDataSet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresultDataSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine_first\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataToAdd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mcombine_first\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   3639\u001b[0m                                      raise_on_error=True)\n\u001b[0;32m   3640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3641\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcombine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcombiner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3642\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3643\u001b[0m     def update(self, other, join='left', overwrite=True, filter_func=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mcombine\u001b[1;34m(self, other, func, fill_value, overwrite)\u001b[0m\n\u001b[0;32m   3535\u001b[0m         \u001b[0mother_idxlen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# save for compare\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3536\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3537\u001b[1;33m         \u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3538\u001b[0m         \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3539\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36malign\u001b[1;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\u001b[0m\n\u001b[0;32m   2677\u001b[0m         return super(DataFrame, self).align(other, join=join, axis=axis, level=level, copy=copy,\n\u001b[0;32m   2678\u001b[0m                                             \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2679\u001b[1;33m                                             fill_axis=fill_axis, broadcast_axis=broadcast_axis)\n\u001b[0m\u001b[0;32m   2680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2681\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reindex'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36malign\u001b[1;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis, broadcast_axis)\u001b[0m\n\u001b[0;32m   3782\u001b[0m                                       \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3783\u001b[0m                                       \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlimit\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3784\u001b[1;33m                                       fill_axis=fill_axis)\n\u001b[0m\u001b[0;32m   3785\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pragma: no cover\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3786\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'unsupported type: %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m_align_series\u001b[1;34m(self, other, join, axis, level, copy, fill_value, method, limit, fill_axis)\u001b[0m\n\u001b[0;32m   3868\u001b[0m                     \u001b[0mfdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlidx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3869\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3870\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Must specify axis=0 or 1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3872\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfdata\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Must specify axis=0 or 1"
     ]
    }
   ],
   "source": [
    "resultDataSet = pd.DataFrame()\n",
    "for source, tech in conf.items():\n",
    "    for tech, param in tech.items():\n",
    "        for filename in os.listdir(downloadpath):\n",
    "            if source in filename:\n",
    "                if tech in filename:\n",
    "                    logger.info('reading')\n",
    "                    logger.info(filename)\n",
    "                    dataToAdd = readData(downloadpath + filename, source, tech)\n",
    "                    resultDataSet = resultDataSet.combine_first(dataToAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Display the Dataset\n",
    "This section can be executed to display a preview of the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resultDataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save csv file to disk\n",
    "Finally, we write the data to csv format and save it in the directory specified in the settings section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultDataSet.to_csv(outputpath+outputfile, sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
