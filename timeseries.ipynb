{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Power System Data\n",
    "## Timeseries data\n",
    "This is a python script that downloads and processes renewables in-feed data from the German TSOs Amprion and TransnetBW.\n",
    "The output is a CSV file containing all the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [Settings](#Settings)\n",
    "\t* [Libraries](#Libraries)\n",
    "* [Downloading the data](#Downloading-the-data)\n",
    "\t* [Parameters for download sources](#Parameters-for-download-sources)\n",
    "\t\t* [ENTSO-E](#ENTSO-E)\n",
    "\t\t* [50Hertz](#50Hertz)\n",
    "\t\t* [Amprion](#Amprion)\n",
    "\t\t* [TransnetBW](#TransnetBW)\n",
    "\t\t* [TenneT](#TenneT)\n",
    "\t* [Downloading files one by one](#Downloading-files-one-by-one)\n",
    "* [Processing the data](#Processing-the-data)\n",
    "\t* [Defining individual read funtions for each data source](#Defining-individual-read-funtions-for-each-data-source)\n",
    "\t\t* [ENTSO-E](#ENTSO-E)\n",
    "\t\t* [50Hertz](#50Hertz)\n",
    "\t\t* [Amprion](#Amprion)\n",
    "\t\t* [TransnetBW](#TransnetBW)\n",
    "\t\t* [TenneT](#TenneT)\n",
    "\t* [Reading files one by one](#Reading-files-one-by-one)\n",
    "\t* [Processing the data](#Processing-the-data)\n",
    "\t* [Display the Dataset](#Display-the-Dataset)\n",
    "* [Save csv file to disk](#Save-csv-file-to-disk)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pytz\n",
    "import yaml\n",
    "import requests\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "logger = logging.getLogger('log')\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters for download sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section contains a python dictionary for each download source with input parameters needed to generate the URLs for the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTSO-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ENTSO = \"\"\"\n",
    "    ENTSO-E: \n",
    "        Data_Portal: \n",
    "            url_template: https://www.entsoe.eu/fileadmin/template/other/statistical_database/excel.php\n",
    "            url_params:\n",
    "                pid: 136\n",
    "                opt_period: 0\n",
    "                send: send\n",
    "                opt_Response: 99\n",
    "                dataindx: 0\n",
    "            url_dates:\n",
    "                opt_Month: '{u_start.month}'\n",
    "                opt_Year: '{u_start.year}'\n",
    "            frequency: M\n",
    "            start: 2006-01-01\n",
    "            end: recent\n",
    "            filetype: xls\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50Hertz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Hertz = \"\"\"\n",
    "    50Hertz: \n",
    "        wind: \n",
    "            url_template: http://ws.50hertz.com/web01/api/WindPowerForecast/DownloadFile\n",
    "            url_params:\n",
    "                callback: '?'\n",
    "            url_dates:\n",
    "                fileName: '{u_start:%Y}.csv'\n",
    "            frequency: A\n",
    "            start: 2005-01-01\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "        pv: \n",
    "            url_template: http://ws.50hertz.com/web01/api/PhotovoltaicForecast/DownloadFile\n",
    "            url_params:\n",
    "                callback: '?'\n",
    "            url_dates:\n",
    "                fileName: '{u_start:%Y}.csv'\n",
    "            frequency: A\n",
    "            start: 2012-01-01\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amprion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Amprion = \"\"\"\n",
    "    Amprion:\n",
    "        wind: \n",
    "            url_template: http://amprion.de/applications/applicationfiles/winddaten2.php\n",
    "            url_params:\n",
    "                mode: download\n",
    "                format: csv\n",
    "            url_dates:\n",
    "                start: '{u_start.day}.{u_start.month}.{u_start.year}'\n",
    "                end: '{u_end.day}.{u_end.month}.{u_end.year}' # dates must not be zero-padded\n",
    "            frequency: complete\n",
    "            start: 2008-01-04\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "        pv: \n",
    "            url_template: http://amprion.de/applications/applicationfiles/PV_einspeisung.php\n",
    "            url_params:\n",
    "                mode: download\n",
    "                format: csv\n",
    "            url_dates:\n",
    "                start: '{u_start.day}.{u_start.month}.{u_start.year}'\n",
    "                end: '{u_end.day}.{u_end.month}.{u_end.year}' # dates must not be zero-padded        \n",
    "            frequency: complete\n",
    "            start: 2010-01-07\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransnetBW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TransnetBW = \"\"\"\n",
    "    TransnetBW: \n",
    "        wind: \n",
    "            url_template: https://www.transnetbw.de/de/kennzahlen/erneuerbare-energien/windenergie\n",
    "            url_params:\n",
    "                app: wind\n",
    "                activeTab: csv\n",
    "                view: 1\n",
    "                download: true\n",
    "            url_dates:\n",
    "                selectMonatDownload: '{u_transnet}'\n",
    "            frequency: M\n",
    "            start: 2010-01-01\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "        pv: \n",
    "            url_template: https://www.transnetbw.de/de/kennzahlen/erneuerbare-energien/fotovoltaik\n",
    "            url_params:\n",
    "                app: wind\n",
    "                activeTab: csv\n",
    "                view: 1\n",
    "                download: true\n",
    "            url_dates:\n",
    "                selectMonatDownload: '{u_transnet}'\n",
    "            frequency: M\n",
    "            start: 2011-01-01\n",
    "            end: recent\n",
    "            filetype: csv\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TenneT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TenneT = \"\"\"\n",
    "    TenneT: \n",
    "        wind: \n",
    "            url_template: http://www.tennettso.de/site/de/phpbridge\n",
    "            url_params:\n",
    "                commandpath: Tatsaechliche_und_prognostizierte_Windenergieeinspeisung/monthDataSheetCsv.php\n",
    "                contenttype: text/x-csv\n",
    "            url_dates:\n",
    "                querystring: monat={u_start:%Y-%m}\n",
    "            frequency: M\n",
    "            start: 2006-01-01\n",
    "            end: recent\n",
    "            filetype: csv        \n",
    "        pv: \n",
    "            url_template: http://www.tennettso.de/site/de/phpbridge\n",
    "            url_params:\n",
    "                commandpath: Tatsaechliche_und_prognostizierte_Solarenergieeinspeisung/monthDataSheetCsv.php\n",
    "                sub: total\n",
    "                contenttype: text/x-csv\n",
    "            url_dates:\n",
    "                 querystring: monat={u_start:%Y-%m}\n",
    "            frequency: M\n",
    "            start: 2010-01-01\n",
    "            end: recent\n",
    "            filetype: csv  \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the parameters for the data sources we wish to include into a YAML-string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = yaml.load(Hertz+Amprion+TenneT+TransnetBW+ENTSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading files one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section creates folders inside the notebook's directory on the users computer for the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "downloadpath = 'downloads1'\n",
    "if not os.path.exists(downloadpath): os.makedirs(downloadpath)\n",
    "archivepath = os.path.join(downloadpath, 'archive-'+datetime.now().strftime('%Y-%m-%d'))\n",
    "archive = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we iterate over the sources and technology (wind/solar) entries specified above and download the data for a the period given in the parameters. The filename is chosen so it reveals the files contents\n",
    "If archive is set to 'True', a copy of each file is also saved under it's original filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download(session, source, tech, s, e, **p):\n",
    "    \"\"\"construct URLs from template and parameters, save, and archive.\"\"\"\n",
    "    logger.info('Attempting download of: {} {} {:%Y-%m-%d}-{:%Y-%m-%d}'.format(source, tech, s, e))\n",
    "    work_file = os.path.join(\n",
    "        downloadpath, source+'_'+tech+'_'+s.strftime('%Y-%m-%d')+'_'+e.strftime('%Y-%m-%d')+'.'+p['filetype']\n",
    "    )        \n",
    "    count = datetime.now().month - s.month + (datetime.now().year - s.year) * 12\n",
    "    for date_key, date_value in p['url_dates'].items():\n",
    "        p['url_params'][date_key] = date_value.format(u_start = s, u_end = e, u_transnet = count)\n",
    "\n",
    "    if not os.path.exists(work_file) or archive == True:\n",
    "        resp = session.get(p['url_template'], params=p['url_params'])                \n",
    "        logger.info('From URL: %s', resp.url)        \n",
    "        save(resp, work_file)\n",
    "        \n",
    "        original_filename = resp.headers['content-disposition'].split('filename=')[-1].replace('\"','').replace(';','')                \n",
    "        logger.info('Archiving under original filename: %s', original_filename)\n",
    "        \n",
    "        full_archivepath = os.path.join(archivepath, source, tech)\n",
    "        if not os.path.exists(full_archivepath): os.makedirs(full_archivepath)\n",
    "        archive_file = os.path.join(full_archivepath,original_filename)\n",
    "        save(resp, archive_file)\n",
    "    else: logger.info('File already downloaded. Skip to next.')        \n",
    "    return\n",
    "\n",
    "def save(resp, filepath):\n",
    "    \"\"\"save a file from a response-object under a given path\"\"\"\n",
    "    if not os.path.exists(filepath):        \n",
    "        with open(filepath, 'wb') as output_file:\n",
    "            for chunk in resp.iter_content(1024):\n",
    "                output_file.write(chunk)\n",
    "    else: logger.info('File already exists. Skip to next.')\n",
    "    return\n",
    "\n",
    "for source, t in conf.items():\n",
    "    for tech, p in t.items():\n",
    "        session = requests.session()\n",
    "#        p['start'] = date(2015,1,1)\n",
    "        if p['end'] == 'recent':\n",
    "            p['end'] = date(2015,12,31)\n",
    "\n",
    "        if p['frequency'] == 'complete':\n",
    "            download(session, source, tech, p['start'], p['end'], **p)            \n",
    "        else:\n",
    "            starts = pd.date_range(start=p['start'], end=p['end'], freq=p['frequency']+'S')\n",
    "            ends = pd.date_range(start=p['start'], end=p['end'], freq=p['frequency'])\n",
    "            for start, end in zip(starts, ends):\n",
    "                download(session, source, tech, start, end, **p)        \n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to merge the downloadet files into one big CSV file. Since every TSO provides the data in a different format, this requires custom read functionality for every source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining individual read funtions for each data source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ENTSO-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_ENTSO(filePath, source, tech):\n",
    "    data = pd.read_excel(\n",
    "        io = filePath,\n",
    "        header=9,\n",
    "#        skiprows = None,\n",
    "        index_col = [0,1],\n",
    "#        parse_cols = None #None means: parse all columns\n",
    "        )\n",
    "    \n",
    "#    dst_transition_days = [d.date() for d in pytz.timezone('Europe/Berlin')._utc_transition_times[1:]]\n",
    "    dst_transition_times = [d.replace(hour=2) for d in pytz.timezone('Europe/Berlin')._utc_transition_times[1:]]\n",
    "    \n",
    "    #the original data has days and countries in the rows and hours in the columns.\n",
    "    #this rearranges the table, mapping hours on the rows and countries on the columns \n",
    "    data = data.stack(level=None).unstack(level='Country').reset_index()    \n",
    "    #pythons DataFrame.stack() puts former columnnames in a new index object named after their level\n",
    "    data.rename(columns={'level_1': 'raw_hour'}, inplace=True)\n",
    "    \n",
    "    #truncate the hours column and replace letters (incating which is which during fall dst-transition)\n",
    "    #hours are indexed 1-24 rather then 0-23, so we deduct 1\n",
    "    data['hour'] = (data['raw_hour'].str[:2].str.replace('A','').str.replace('B','').astype(int) - 1).astype(str)    \n",
    "    data['dt_index'] = pd.to_datetime(data['Day']+' '+data['hour']+':00')\n",
    "    data.set_index('dt_index', inplace=True)    \n",
    "    \n",
    "    # drop 2nd occurence of 03:00 appearing in October data except for autumn dst-transition\n",
    "    data = data[~((data['raw_hour'] == '3B:00:00') & ~(data.index.isin(dst_transition_times)))]\n",
    "    #drop 03:00 for (spring) dst-transition. October data is unaffected because the format is 3A:00/3B:00 \n",
    "    data = data[~((data['raw_hour'] == '03:00:00') & (data.index.isin(dst_transition_times)))]\n",
    "    \n",
    "    data.index = data.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    data.drop(['Day', 'hour', 'raw_hour'], axis=1, inplace = True)\n",
    "    data.rename(columns=lambda x: 'load_'+x, inplace=True)\n",
    "    data = data.replace(to_replace='n.a.', value=np.nan)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50Hertz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_50Hertz(filePath, source, tech):\n",
    "    data = pd.read_csv(\n",
    "        filePath,\n",
    "        sep = \";\",\n",
    "        header = 3,\n",
    "        index_col = 'dt_index',\n",
    "        names = ['input_date', 'input_time', source+'_'+tech+'_actual'],\n",
    "        parse_dates = {'dt_index' : ['input_date', 'input_time']},\n",
    "        date_parser = None,\n",
    "        dayfirst = True,\n",
    "        decimal = ',',\n",
    "        thousands = '.',\n",
    "        converters = {'input_time' : lambda x: x[:5]}, # truncate values in 'input_time' column after 5th character\n",
    "        usecols = [0,1,3],\n",
    "#        engine = 'c', # only the C-engine allows to set the dtype parameter\n",
    "#        dtype = 'str', # python uses float as the datatype, this sometimes causes changes in the last decimal place\n",
    "    )\n",
    "    \n",
    "    if pd.to_datetime(data.index.values[0]).year not in range(2007,2015):\n",
    "    # Until 2006 as well as  in 2015, only the wintertime October dst-transition (marked by a B in the data) is reported,\n",
    "    # the summertime hour, (marked by an A) is missing in the data \n",
    "        dst_arr = np.zeros(len(data.index), dtype=bool)\n",
    "        data.index = data.index.tz_localize('Europe/Berlin', ambiguous=dst_arr)\n",
    "    else:\n",
    "        data.index = data.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    \n",
    "    return data            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amprion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_Amprion(filePath, source, tech):\n",
    "    data = pd.read_csv(\n",
    "        filePath,\n",
    "        sep = \";\",\n",
    "        header = 0,\n",
    "        index_col = 'dt_index',\n",
    "        names = ['input_date', 'input_time', source+'_'+tech+'_forecast', source+'_'+tech+'_actual'],\n",
    "        parse_dates = {'dt_index' : ['input_date', 'input_time']},\n",
    "        date_parser = None,\n",
    "        dayfirst = True,\n",
    "#        decimal = ',', #shouldn't be relevant\n",
    "        thousands = None,\n",
    "        converters = {'input_time' : lambda x: x[:5]}, # truncate values in 'input_time' column after 5th character\n",
    "        usecols = [0,1,2,3],        \n",
    "    )\n",
    "\n",
    "    index1 = data.index[data.index.year <= 2009]\n",
    "    index1 = index1.tz_localize('Europe/Berlin', ambiguous='infer')        \n",
    "    index2 = data.index[data.index.year > 2009]\n",
    "    dst_arr = np.ones(len(index2), dtype=bool)\n",
    "    index2 = index2.tz_localize('Europe/Berlin', ambiguous=dst_arr)        \n",
    "    data.index = index1.append(index2)\n",
    "\n",
    "    # dst_arr is a boolean array consisting only of \"True\" entries, \n",
    "    # telling python to treat the hour from 2:00 to 2:59 as summertime\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TransnetBW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_TransnetBW(filePath, source, tech):\n",
    "    data = pd.read_csv(\n",
    "        filePath,\n",
    "        sep = \";\",\n",
    "        header = 0,\n",
    "        index_col = \"dt_index\",\n",
    "        names = ['input_date', 'input_time', source+'_'+tech+'_forecast', source+'_'+tech+'_actual'],\n",
    "        parse_dates = {'dt_index' : ['input_date', 'input_time',]},\n",
    "        date_parser = None,         \n",
    "        dayfirst = True,\n",
    "        decimal = ',',\n",
    "        thousands = None,\n",
    "        converters = None,\n",
    "        usecols = [0,1,4,5],\n",
    "    )\n",
    "        \n",
    "    data.index = data.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "    # ambigous refers to how the October dst-transition hour is handled.\n",
    "    # ‘infer’ will attempt to infer dst-transition hours based on order\n",
    "        \n",
    "    return data  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TenneT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_TenneT(filePath, source, tech):\n",
    "    data = pd.read_csv(\n",
    "        filePath,\n",
    "        sep = \";\",\n",
    "        encoding = 'latin_1',\n",
    "        header = 3,\n",
    "        index_col = None,\n",
    "        names=['input_date', 'position', source+'_'+tech+'_forecast', source+'_'+tech+'_actual', source+'_'+tech+'_offshore_share'],\n",
    "        parse_dates = False,\n",
    "        date_parser = None,\n",
    "        dayfirst = True,\n",
    "#       decimal = ',', #shouldn't be relevant\n",
    "        thousands = None,\n",
    "        converters = None,          \n",
    "        usecols = [0,1,2,3,4],\n",
    "    )\n",
    "\n",
    "    data['input_date'].fillna(method='ffill', limit = 100, inplace=True)\n",
    "\n",
    "    # The Tennet Data doesn't feature a timestamp column. Instead, the quarter-hourly data entries for each day are numbered\n",
    "    # by their position, creating an index ranging from 1 to 96 on normal days. This index can be used to compute a timestamp.\n",
    "    # However, the behaviour for DST switch dates needs to be specified seperately as follows:\n",
    "\n",
    "    for i in range(len(data.index)):\n",
    "        # On the day in March when summertime begins, shift the data forward by 1 hour,\n",
    "        # beginning with the 9th quarter-hour, so the index runs again up to 96\n",
    "        if data['position'][i] == 92 and ((i == len(data.index)-1) or (data['position'][i+1] == 1)):\n",
    "            slicer = data[(data['input_date'] == data['input_date'][i]) & (data['position'] >= 9)].index\n",
    "            data.loc[slicer, 'position'] = data['position'] + 4\n",
    "\n",
    "        if data['position'][i] > 96: # True when summertime ends in October\n",
    "            logger.info('%s th quarter-hour at %s, position %s',data['position'][i], data.ix[i,'input_date'], (i))  \n",
    "\n",
    "            # Instead of having the quarter-hours' index run up to 100, we want to have\n",
    "            # it set back by 1 hour beginning from the 13th quarter-hour, ending at 96\n",
    "            if data['position'][i] == 100 and not (data['position'] == 101).any():\n",
    "                slicer = data[(data['input_date'] == data['input_date'][i]) & (data['position'] >= 13)].index\n",
    "                data.loc[slicer, 'position'] = data['position'] - 4                     \n",
    "\n",
    "            # in 2011 and 2012, there are 101 qaurter hours on the day the summertime ends, so 1 too many.\n",
    "            # From looking at the data, we inferred that the 13'th quarter hour is the culprit, so we drop that.\n",
    "            # The following entries for that day need to be shifted \n",
    "            elif data['position'][i] == 101: \n",
    "                data = data[~((data['input_date'] == data['input_date'][i]) & (data['position'] == 13))]\n",
    "                slicer = data[(data['input_date'] == data['input_date'][i]) & (data['position'] >= 13)].index\n",
    "                data.loc[slicer, 'position'] = data['position'] - 5         \n",
    "\n",
    "    # On 2012-03-25, there are 94 entries, where entries 8 and 10 are probably wrong\n",
    "    if data['input_date'][0] == '2012-03-01':\n",
    "        data = data[~((data['input_date'] == '2012-03-25') & ((data['position'] == 8) | (data['position'] == 10)))]\n",
    "        slicer = data[(data['input_date'] == '2012-03-25') & (data['position'] >= 9)].index\n",
    "        data.loc[slicer, 'position'] = [8] + list(range(13, 97))        \n",
    "\n",
    "    # On 2012-09-27, there are 97 entries. Probably, just the 97th entry is wrong\n",
    "    if data['input_date'][0] == '2012-09-01':\n",
    "        data = data[~((data['input_date'] == '2012-09-27') & (data['position'] == 97))]          \n",
    "\n",
    "    # Here we compute the timestamp from the position and generate the datetime-index\n",
    "    data['hour'] = (np.trunc((data['position']-1)/4)).astype(int).astype(str)\n",
    "    data['minute'] = (((data['position']-1)%4)*15).astype(int).astype(str)\n",
    "    data['dt_index'] = pd.to_datetime(data['input_date']+' '+data['hour']+':'+data['minute'], dayfirst = True)\n",
    "    data.set_index('dt_index',inplace=True)\n",
    "\n",
    "    # In the years 2006, 2008, and 2009, the dst-transition hour in March appears as empty rows in the data.\n",
    "    # We delete it from the set in order to make the timezone localization work\n",
    "    for crucial_date in pd.to_datetime(['2006-03-26','2008-03-30','2009-03-29']).date:\n",
    "        if data.index[0].year == crucial_date.year:\n",
    "            data = data[~((data.index.date == crucial_date) & (data.index.hour == 2))]\n",
    "\n",
    "    data.index = data.index.tz_localize('Europe/Berlin', ambiguous='infer')\n",
    "\n",
    "#    data = data.drop(['position', 'input_date', 'hour', 'minute'], axis=1)\n",
    "    if tech == 'wind':\n",
    "        data = data[[source+'_'+tech+'_forecast', source+'_'+tech+'_actual', source+'_'+tech+'_offshore_share']]\n",
    "    if tech == 'pv':\n",
    "        data = data[[source+'_'+tech+'_forecast', source+'_'+tech+'_actual',]]\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an empty DataFrame / reset the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resultDataSet = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each source/TSO and technology specified in the conf dict, this section finds all the downloaded files in the downloads folder and then calls the matching readData function.\n",
    "The datasets returned by the read function are then merged into one large dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = yaml.load(Hertz+Amprion+TenneT+TransnetBW+ENTSO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for source, tech in conf.items():\n",
    "    for tech, param in tech.items():\n",
    "        for filename in os.listdir(downloadpath):\n",
    "            if source in filename and tech in filename:\n",
    "                logger.info('reading %s', filename)\n",
    "                if os.path.getsize(os.path.join(downloadpath, filename)) < 128:\n",
    "                    logger.info(\"file is smaller than 128 Byte, which means it's probably empty\")\n",
    "                else:                    \n",
    "                    if source == 'ENTSO-E':\n",
    "                        dataToAdd = read_ENTSO(os.path.join(downloadpath, filename), source, tech)\n",
    "                    elif source == 'Svenska_Kraftnaet':\n",
    "                        dataToAdd = read_Svenska_Kraftnaet(os.path.join(downloadpath, filename), source, tech)\n",
    "                    elif source == '50Hertz':\n",
    "                        dataToAdd = read_50Hertz(os.path.join(downloadpath, filename), source, tech)\n",
    "                    elif source == 'Amprion':\n",
    "                        dataToAdd = read_Amprion(os.path.join(downloadpath, filename), source, tech)\n",
    "                    elif source == 'TenneT':\n",
    "                        dataToAdd = read_TenneT(os.path.join(downloadpath, filename), source, tech)\n",
    "                    elif source == 'TransnetBW':\n",
    "                        dataToAdd = read_TransnetBW(os.path.join(downloadpath, filename), source, tech)\n",
    "\n",
    "                    resultDataSet = resultDataSet.combine_first(dataToAdd)\n",
    "#                    resultDataSet.update(dataToAdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the renewables in-feed data comes in 15-minute intervals. We resample it to hourly intervals in order to match the load data from ENTSO-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "resampledDataSet = resultDataSet.resample('H', how='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The in-feed data for the 4 German controll areas is summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resampledDataSet['wind_DE'] = (\n",
    "    resampledDataSet['50Hertz_wind_actual'] +\n",
    "    resampledDataSet['Amprion_wind_actual'] +\n",
    "    resampledDataSet['TransnetBW_wind_actual'] +\n",
    "    resampledDataSet['TenneT_wind_actual']\n",
    "    )    \n",
    "resampledDataSet['pv_DE'] = (\n",
    "    resampledDataSet['50Hertz_pv_actual'] +\n",
    "    resampledDataSet['Amprion_pv_actual'] +\n",
    "    resampledDataSet['TransnetBW_pv_actual'] +\n",
    "    resampledDataSet['TenneT_pv_actual']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section can be executed to display a preview of the merged dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resampledDataSet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save csv file to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the data to CSV format and save it in the directory of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resampledDataSet.to_csv('output1.csv', sep=';', float_format='%.2f', decimal=',', date_format='%Y-%m-%dT%H:%M:%S%z')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
