{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "#import pytz\n",
    "#import yaml\n",
    "#import os\n",
    "#import numpy as np\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pycountry\n",
    "import json\n",
    "#from collections import OrderedDict\n",
    "import sqlite3\n",
    "import copy\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset created by the read-script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "for resolution in ['15min', '60min']:\n",
    "    data_sets[resolution] = pd.read_csv(\n",
    "        'raw_data_' + resolution + '.csv',\n",
    "        header=[0,1,2,3,4],\n",
    "        index_col=0, #'timestamp',\n",
    "        parse_dates=True #'timestamp',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating German data from individual TSOs\n",
    "The in-feed data for the 4 German controll areas is summed up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADERS = ['variable', 'country', 'attribute', 'source', 'web']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web = 'http://data.open-power-system-data.org/datapackage_timeseries'\n",
    "for tech in ['wind', 'pv']:\n",
    "    for attribute in ['generation', 'forecast']:\n",
    "        sum_col = pd.Series()\n",
    "        for tso in ['50hertz', 'amprion', 'tennet', 'transnetbw']:\n",
    "            add_col = data_sets['15min'][tech, 'DE' + tso, attribute]\n",
    "            if len(sum_col) == 0:\n",
    "                sum_col = add_col\n",
    "            else:\n",
    "                sum_col = sum_col + add_col.values\n",
    "                \n",
    "        # Create a new MultiIndex\n",
    "        tuples = [(tech, 'DE', attribute, 'own_calculation', web)]\n",
    "        columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "        sum_col.columns = columns\n",
    "        data_sets['15min'] = data_sets['15min'].combine_first(sum_col)\n",
    "        \n",
    "        # Calculate the profile column\n",
    "        if attribute == 'generation':\n",
    "            profile_col = sum_col.values / data_sets['15min'][tech, 'DE', 'capacity']\n",
    "            tuples = [(tech, 'DE', 'profile', 'own_calculation', web)]\n",
    "            columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "            profile_col.columns = columns\n",
    "            data_sets['15min'] = data_sets['15min'].combine_first(profile_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hourly data from 15' data\n",
    "Most of the renewables in-feed data comes in 15-minute intervals. We resample it to hourly intervals in order to match the load data from ENTSO-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resampled = data_sets['15min'].resample('H').mean()\n",
    "data_sets['60min'] = data_sets['60min'].combine_first(resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data that is available in 15 minute resolution is extracted to be saved separately"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save csv file to disk\n",
    "Finally, we write the data to CSV format and save it in the directory of this notebook. Two files are created: one in hourly granularity called \"timeseries60.csv\" (including all data); and one in quarter-hourly granularity called \"timeseries15.csv\" (including only data avaiable at this resultion)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the metadata\n",
    "In this part, we create the metadata that will document the data output in CSV format. The metadata we be stored in JSON format, which is very much like a python dictionary.\n",
    "## General metadata\n",
    "First, we define the general metadata for the timeseries datapackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'name': 'opsd-timeseries',\n",
    "    'title': 'Time-series data: load, wind and solar, prices',\n",
    "    'description': 'This dataset contains timeseries data of wind and solar ' +\n",
    "        'in-feed into the grids of German Transmission System Operators ' +\n",
    "        'as well as load timeseries for 37 European countries from ENTSO-E.',\n",
    "    'opsd-jupyter-notebook-url': 'https://github.com/Open-Power-System-Data/' +\n",
    "        'datapackage_timeseries/blob/master/main.ipynb',\n",
    "    'version': '2016-03-13',\n",
    "    'opsd-changes-to-last-version': 'Various things...',\n",
    "    'keywords': [\n",
    "        'timeseries','electricity','in-feed','capacity','renewables', 'wind',\n",
    "        'solar','load','tso','europe','germany'\n",
    "        ],\n",
    "    'geographical-scope': 'Europe/Germany',\n",
    "    'licenses': [{\n",
    "        'url': 'http://example.com/license/url/here',\n",
    "        'version': '1.0',\n",
    "        'name': 'License Name Here',\n",
    "        'id': 'license-id-from-open'\n",
    "        }],\n",
    "    'sources': [],\n",
    "    'maintainers': [{\n",
    "        'web': 'http://example.com/',\n",
    "        'name': 'Jonathan Muehlenpfordt',\n",
    "        'email': 'muehlenpfordt@neon-energie.de'\n",
    "        }],\n",
    "    'resources': [{ # The following is an example of how the file-specific metadata is \n",
    "        'path': 'path_to.csv', # structured. The actual metadata is created below\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {\n",
    "            'fields': [{\n",
    "                'name': 'load_AT_actual',\n",
    "                'description': 'Consumption in Austria in MW',\n",
    "                'type': 'number',\n",
    "                'source': {\n",
    "                    'name': 'Example',\n",
    "                    'web': 'http://www.example.com'\n",
    "                    },\n",
    "                'opsd-properties': {\n",
    "                    'Country': 'AT',\n",
    "                    'Variable': 'load',\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "indexfield = {\n",
    "    'name': 'timestamp',\n",
    "    'description': 'Start of timeperiod in UTC',\n",
    "    'type': 'datetime',\n",
    "    'format': 'YYYY-MM-DDThh:mm:ssZ'\n",
    "    }\n",
    "\n",
    "descriptions = {\n",
    "    'net': 'Consumption in {geo} in MW',\n",
    "    'generation': 'Actual {tech} generation in {geo} in MW',\n",
    "    'forecast': '{tech} day-ahead generation forecast in {geo} in MW',\n",
    "    'capacity': '{tech} capacity in {geo} in MW',\n",
    "    'profile': 'Share of {tech} capacity producing in {geo}',\n",
    "    'offshoreshare': '{tech} actual offshore generation in {geo} in MW'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns specific metadata\n",
    "For each dataset/outputfile, the metadata has an entry in the \"resources\" list that describes the file/dataset. The main part of each entry is the \"schema\" dictionary, consisting of a list of 'fields\", meaning the columns in the dataset. The first field is the timestamp index of the dataset. For the other fields, we iterate over the columns of the MultiIndex index of the datasets to contruct the each field's metadata.\n",
    "\n",
    "At the same time, a copy of the datasets is created that has a single line column index instead of the MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sets_singleindex = copy.deepcopy(data_sets)##########################\n",
    "resources = []\n",
    "for res_key, data_set in data_sets.items():\n",
    "    columns_singleindex = [] ####################\n",
    "    fields = [indexfield]\n",
    "    for col in data_set.columns:\n",
    "        h = {k: v for k, v in zip(HEADERS, col)}\n",
    "        if len(h['country']) > 2:\n",
    "            geo = h['country'] + ' control area'\n",
    "        elif h['country'] == 'NI':\n",
    "            geo = 'Northern Ireland'\n",
    "        elif h['country'] == 'CS':\n",
    "            geo = 'Serbia and Montenegro'\n",
    "        else:\n",
    "            geo = pycountry.countries.get(alpha2=h['country']).name\n",
    "\n",
    "        field = {}    \n",
    "        field['description'] = descriptions[h['attribute']].format(\n",
    "            tech=h['variable'], geo=geo)\n",
    "        field['type'] = 'number'\n",
    "        field['source'] = {\n",
    "            'name': h['source'],\n",
    "            'web': h['web']\n",
    "            }\n",
    "        field['opsd-properties'] = {\n",
    "            'Country': h['country'],\n",
    "            'Variable': h['variable'],\n",
    "            }\n",
    "        components = [h['variable'], h['country']]\n",
    "        if not h['variable'] == 'load':\n",
    "            components.append(h['attribute'])\n",
    "            field['opsd-properties']['Attribute'] = h['attribute']\n",
    "        field['name'] = '_'.join(components)\n",
    "        columns_singleindex.append(field['name'])\n",
    "        fields.append(field)\n",
    "        \n",
    "    resource = {\n",
    "        'path': 'timeseries' + res_key + '.csv',\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {'fields': fields}\n",
    "        }       \n",
    "    resources.append(resource)\n",
    "    data_sets_singleindex[res_key].columns = columns_singleindex ###############\n",
    "    \n",
    "metadata['resources'] = resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fieldstub = {\n",
    "    'name': 'same as above',\n",
    "    'description': '',\n",
    "    'type': '',\n",
    "    'format': ''\n",
    "    }\n",
    "\n",
    "resources2 = resources\n",
    "for res_key in ['15min', '60min']:\n",
    "    more_resources = [{\n",
    "        'path': 'timeseries' + res_key + '.xlsx',\n",
    "        'format': 'xlsx',\n",
    "        'mediatype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "        'schema': {'fields': [fieldstub]}\n",
    "        },\n",
    "        {\n",
    "        'path': 'timeseries' + res_key + '_multiindex.xlsx',\n",
    "        'format': 'xlsx',\n",
    "        'mediatype': 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',\n",
    "        'schema': {'fields': [fieldstub]}\n",
    "        },\n",
    "        {\n",
    "        'path': 'timeseries' + res_key + '_multiindex.csv',\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {'fields': [fieldstub]}\n",
    "        },\n",
    "        {\n",
    "        'path': 'timeseries' + res_key + '_stacked.csv',\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {'fields': [fieldstub]}\n",
    "        }]\n",
    "    resources2.extend(more_resources)\n",
    "metadata['resources'] = resources2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to write the metadata to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapackage_json = json.dumps(metadata, indent=2, separators=(',', ': '))\n",
    "with open('datapackage.json', 'w') as f:\n",
    "    f.write(datapackage_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_sets_singleindex = copy.deepcopy(data_sets)\n",
    "for res_key, data_set in data_sets.items():\n",
    "    columns_singleindex = []\n",
    "    for col in data_set.columns:\n",
    "        h = {k: v for k, v in zip(COLUMN_HEADERS, col)}\n",
    "        if h['source'] in ['50Hertz', 'Amprion', 'TenneT', 'TransnetBW']:\n",
    "            h['country'] = h['country'] + h['source'].lower()            \n",
    "        col_singleindex = h['variable'] + '_' + h['country'] + '_' + h['attribute']\n",
    "        columns_singleindex.append(col_singleindex)\n",
    "    data_sets_singleindex[res_key].columns = columns_singleindex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing the data to disk\n",
    "We prepare different schemas for the data output. Those are "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets_multiindex = {}\n",
    "data_sets_stacked = {}\n",
    "for res_key in ['15min', '60min']:\n",
    "    data_sets_multiindex[res_key + '_multiindex'] = data_sets[res_key]\n",
    "    \n",
    "    stacked = data_sets[res_key].copy()\n",
    "    stacked.columns = stacked.columns.droplevel(['source', 'web'])\n",
    "    stacked = stacked.transpose().stack(dropna=True).to_frame(name='data')\n",
    "    data_sets_stacked[res_key + '_stacked'] = stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to SQL-database\n",
    "This file format is required for the filtering funtion on the OPSD website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for res_key, data_set in data_sets_singleindex.items():\n",
    "    f = 'timeseries' + res_key\n",
    "    ds = data_set.copy()\n",
    "    ds.index = ds.index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    ds.to_sql(f, sqlite3.connect(f + '.sqlite'),\n",
    "              if_exists='replace', index_label='timestamp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to Excel\n",
    "Warning: This takes about 15 Minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for res_key, data_set in chain(data_sets_singleindex.items(),\n",
    "                               data_sets_multiindex.items()):\n",
    "    if res_key == '60min_multiindex':\n",
    "        f = 'timeseries' + res_key\n",
    "        data_set.to_excel(f+ '.xlsx', float_format='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for res_key, data_set in chain(data_sets_singleindex.items(),\n",
    "                               data_sets_multiindex.items()):\n",
    "                               #data_sets_stacked.items()):\n",
    "    f = 'timeseries' + res_key\n",
    "    data_set.to_csv(f + '.csv', float_format='%.2f',\n",
    "                    date_format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing data handling\n",
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for col in data_set.columns:\n",
    "#    df = col.to_frame()\n",
    "df = data_sets['15min'].iloc[:,20].to_frame()\n",
    "df['tag'] = ((df.index >= df.first_valid_index()) &\n",
    "             (df.index <= df.last_valid_index()) &\n",
    "             df.isnull().transpose().as_matrix()).transpose()\n",
    "\n",
    "# make another DF to hold info about each region\n",
    "regs_isnull = pd.DataFrame()\n",
    "\n",
    "# first row of consecutive region is a True preceded by a False in tags\n",
    "regs_isnull['start_idx']  = df.index[df['tag'] & ~ df['tag'].shift(1).fillna(False)]\n",
    "\n",
    "# last row of consecutive region is a False preceded by a True   \n",
    "regs_isnull['end_idx']  = df.index[df['tag'] & ~ df['tag'].shift(-1).fillna(False)] \n",
    "\n",
    "if df['tag'].any():\n",
    "    # how long is each region\n",
    "    regs_isnull['spans'] = regs_isnull['end_idx'] - regs_isnull['start_idx'] + timedelta(minutes=15)\n",
    "    \n",
    "    # index of the region with the longest span      \n",
    "    max_idx = regs_isnull['spans'].argmax()\n",
    "\n",
    "    # we can get the start and end points of longest region from the original dataframe \n",
    "    df.ix[regs_isnull.ix[max_idx][['start_idx', 'end_idx']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "regs_isnull\n",
    "df.ix[regs_isnull.ix[max_idx][['start_idx', 'end_idx']].values]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
