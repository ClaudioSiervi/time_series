{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing\n",
    "Part of the project [Open Power System Data](http://open-power-system-data.org/).\n",
    "\n",
    "Go back to the main notebook ([GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/main.ipynb?) / [local copy](main.ipynb#))\n",
    "\n",
    "This notebook processes the data combined by the read script ([GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/read.ipynb) / [local copy](read.ipynb#))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Preparations](#1.-Preparations)\n",
    "\t* [1.1 Libraries](#1.1-Libraries)\n",
    "\t* [1.2 Load raw data](#1.2-Load-raw-data)\n",
    "* [2. Own calculations](#2.-Own-calculations)\n",
    "\t* [2.1 Aggregate German data from individual TSOs](#2.1-Aggregate-German-data-from-individual-TSOs)\n",
    "\t* [2.2 Create hourly data from 15' data](#2.2-Create-hourly-data-from-15'-data)\n",
    "* [3. Create metadata](#3.-Create-metadata)\n",
    "\t* [3.1 General metadata](#3.1-General-metadata)\n",
    "\t* [3.2 Columns-specific metadata](#3.2-Columns-specific-metadata)\n",
    "* [4. Writing the data to disk](#4.-Writing-the-data-to-disk)\n",
    "\t* [4.1 Write to SQL-database](#4.1-Write-to-SQL-database)\n",
    "\t* [4.2 Write to Excel](#4.2-Write-to-Excel)\n",
    "\t* [4.3 Write to CSV](#4.3-Write-to-CSV)\n",
    "* [5. Missing data handling](#5.-Missing-data-handling)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading some python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import logging\n",
    "import pycountry\n",
    "import json\n",
    "import sqlite3\n",
    "import copy\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Load raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset compiled by the read-script ([local copy](read.ipynb#) / [GitHub](https://github.com/Open-Power-System-Data/datapackage_timeseries/blob/master/read.ipynb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets = {}\n",
    "for res_key in ['15min', '60min']:\n",
    "    data_sets[res_key] = pd.read_csv(\n",
    "        'raw_data_' + res_key + '.csv',\n",
    "        header=[0,1,2,3,4],\n",
    "        index_col=0,\n",
    "        parse_dates=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Own calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Aggregate German data from individual TSOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wind and solar in-feed data for the 4 German balancing areas is summed up and stored in in new columns, which are then used to calculate profiles, that is, the share of wind/solar capacity producing at a given time. The column headers are created in the fashion introduced in the read script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HEADERS = ['variable', 'country', 'attribute', 'source', 'web']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "web = 'http://data.open-power-system-data.org/datapackage_timeseries'\n",
    "for tech in ['wind', 'solar']:\n",
    "    for attribute in ['generation', 'forecast']:\n",
    "        sum_col = pd.Series()\n",
    "        for tso in ['50hertz', 'amprion', 'tennet', 'transnetbw']:\n",
    "            add_col = data_sets['15min'][tech, 'DE' + tso, attribute]\n",
    "            if len(sum_col) == 0:\n",
    "                sum_col = add_col\n",
    "            else:\n",
    "                sum_col = sum_col + add_col.values\n",
    "                \n",
    "        # Create a new MultiIndex\n",
    "        tuples = [(tech, 'DE', attribute, 'own calculation', web)]\n",
    "        columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "        sum_col.columns = columns\n",
    "        data_sets['15min'] = data_sets['15min'].combine_first(sum_col)\n",
    "        \n",
    "        # Calculate the profile column\n",
    "        if attribute == 'generation':\n",
    "            profile_col = sum_col.values / data_sets['15min'][tech, 'DE', 'capacity']\n",
    "            tuples = [(tech, 'DE', 'profile', 'own calculation', web)]\n",
    "            columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "            profile_col.columns = columns\n",
    "            data_sets['15min'] = data_sets['15min'].combine_first(profile_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Create hourly data from 15' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German renewables in-feed data comes in 15-minute intervals. We resample it to hourly intervals in order to match the load data from ENTSO-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resampled = data_sets['15min'].resample('H').mean()\n",
    "data_sets['60min'] = data_sets['60min'].combine_first(resampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we create the metadata that will document the data output in CSV format. The metadata we be stored in JSON format, which is very much like a python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 General metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the general metadata for the timeseries datapackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'name': 'opsd-timeseries',\n",
    "    'title': 'Time-series data: load, wind and solar, prices',\n",
    "    'description': 'This data package contains different kinds of timeseries ' +\n",
    "        'data relevant for power system modelling. Currently, the data ' + \n",
    "        'includes hourly electricity consumption (load) from ENTSO-E for 36 ' +\n",
    "        'European countries, wind and solar power generation from German ' +\n",
    "        'transmission system operators 50Hertz, Amprion, TenneT and '+\n",
    "        'TransnetBW for every quarter hour, and daily wind and solar ' +\n",
    "        'capacity data from Netztransparenz.de and Bundesnetzagentur. We use ' +\n",
    "        'this data to calculate Germany-wide renewables in-feed and profile ' +\n",
    "        'timeseries. While the some of the wind in-feed data dates back to ' +\n",
    "        '2005, the full dataset is only available from 2012 onwards. The ' +\n",
    "        'data has been downloaded from the sources, resampled and merged in ' +\n",
    "        'a large CSV file with hourly resolution. Additionally, the data ' +\n",
    "        'available at a higher resolution (German renewables in-feed, 15 ' +\n",
    "        'minutes) is provided in a separate file. All data processing is ' +\n",
    "        'conducted in python and pandas and has been documented in the ' +\n",
    "        'Jupyter notebooks linked below.',\n",
    "    'opsd-jupyter-notebook-url': 'https://github.com/Open-Power-System-Data/' +\n",
    "        'datapackage_timeseries/blob/master/main.ipynb',\n",
    "    'version': '2016-03-18',\n",
    "    'opsd-changes-to-last-version': 'Introduced various formats for output data',\n",
    "    'keywords': [\n",
    "        'timeseries','electricity','in-feed','capacity','renewables', 'wind',\n",
    "        'solar','load','tso','europe','germany'\n",
    "        ],\n",
    "    'geographical-scope': 'Europe/Germany',\n",
    "    'licenses': [{\n",
    "        'url': 'http://example.com/license/url/here',\n",
    "        'version': '1.0',\n",
    "        'name': 'License Name Here',\n",
    "        'id': 'license-id-from-open'\n",
    "        }],\n",
    "    'views': [{}],\n",
    "    'sources': [{\n",
    "        'name': 'See the \"Source\" column in the field documentation'\n",
    "        }],\n",
    "    'maintainers': [{\n",
    "        'web': 'http://example.com/',\n",
    "        'name': 'Jonathan Muehlenpfordt',\n",
    "        'email': 'muehlenpfordt@neon-energie.de'\n",
    "        }],\n",
    "    'resources': [{ # The following is an example of how the file-specific metadata is \n",
    "        'path': 'path_to.csv', # structured. The actual metadata is created below\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {\n",
    "            'fields': [{\n",
    "                'name': 'load_AT_actual',\n",
    "                'description': 'Consumption in Austria in MW',\n",
    "                'type': 'number',\n",
    "                'source': {\n",
    "                    'name': 'Example',\n",
    "                    'web': 'http://www.example.com'\n",
    "                    },\n",
    "                'opsd-properties': {\n",
    "                    'Country': 'AT',\n",
    "                    'Variable': 'load',\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "indexfield = {\n",
    "    'name': 'timestamp',\n",
    "    'description': 'Start of timeperiod in UTC',\n",
    "    'type': 'datetime',\n",
    "    'format': 'YYYY-MM-DDThh:mm:ssZ'\n",
    "    }\n",
    "\n",
    "descriptions = {\n",
    "    'load': 'Consumption in {geo} in MW',\n",
    "    'generation': 'Actual {tech} generation in {geo} in MW',\n",
    "    'forecast': '{tech} day-ahead generation forecast in {geo} in MW',\n",
    "    'capacity': '{tech} capacity in {geo} in MW',\n",
    "    'profile': 'Share of {tech} capacity producing in {geo}',\n",
    "    'offshoreshare': '{tech} actual offshore generation in {geo} in MW'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Columns-specific metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset/outputfile, the metadata has an entry in the \"resources\" list that describes the file/dataset. The main part of each entry is the \"schema\" dictionary, consisting of a list of \"fields\", meaning the columns in the dataset. The first field is the timestamp index of the dataset. For the other fields, we iterate over the columns of the MultiIndex index of the datasets to contruct the corresponding metadata.\n",
    "\n",
    "At the same time, a copy of the datasets is created that has a single line column index instead of the MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_sets_singleindex = copy.deepcopy(data_sets)##########################\n",
    "resources = []\n",
    "for res_key, data_set in data_sets.items():\n",
    "    columns_singleindex = [] ####################\n",
    "    fields = [indexfield]\n",
    "    for col in data_set.columns:\n",
    "        h = {k: v for k, v in zip(HEADERS, col)}\n",
    "        if len(h['country']) > 2:\n",
    "            geo = h['country'] + ' control area'\n",
    "        elif h['country'] == 'NI':\n",
    "            geo = 'Northern Ireland'\n",
    "        elif h['country'] == 'CS':\n",
    "            geo = 'Serbia and Montenegro'\n",
    "        else:\n",
    "            geo = pycountry.countries.get(alpha2=h['country']).name\n",
    "\n",
    "        field = {}    \n",
    "        field['description'] = descriptions[h['attribute']].format(\n",
    "            tech=h['variable'], geo=geo)\n",
    "        field['type'] = 'number'\n",
    "        field['source'] = {\n",
    "            'name': h['source'],\n",
    "            'web': h['web']\n",
    "            }\n",
    "        field['opsd-properties'] = {\n",
    "            'Country': h['country'],\n",
    "            'Variable': h['variable'],\n",
    "            }\n",
    "        components = [h['variable'], h['country']]\n",
    "        if not h['variable'] == 'load':\n",
    "            components.append(h['attribute'])\n",
    "            field['opsd-properties']['Attribute'] = h['attribute']\n",
    "        field['name'] = '_'.join(components)\n",
    "        columns_singleindex.append(field['name'])\n",
    "        fields.append(field)\n",
    "        \n",
    "    resource = {\n",
    "        'path': 'timeseries' + res_key + '.csv',\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'alternative_formats': [\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '.csv',\n",
    "          'stacking': 'Singleindex',\n",
    "          'format': 'csv'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '.xlsx',\n",
    "          'stacking': 'Singleindex',\n",
    "          'format': 'xlsx'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '_multiindex.xlsx',\n",
    "          'stacking': 'Multiindex',\n",
    "          'format': 'xlsx'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '_multiindex.csv',\n",
    "          'stacking': 'Multiindex',\n",
    "          'format': 'csv'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '_stacked.csv',\n",
    "          'stacking': 'Stacked',\n",
    "          'format': 'csv'\n",
    "        }\n",
    "      ],        \n",
    "        'schema': {'fields': fields}\n",
    "        }       \n",
    "    resources.append(resource)\n",
    "    data_sets_singleindex[res_key].columns = columns_singleindex ###############\n",
    "    \n",
    "metadata['resources'] = resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to write the metadata to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapackage_json = json.dumps(metadata, indent=2, separators=(',', ': '))\n",
    "with open('datapackage.json', 'w') as f:\n",
    "    f.write(datapackage_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Writing the data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the data to CSV format and save it in the directory of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets_multiindex = {}\n",
    "data_sets_stacked = {}\n",
    "for res_key in ['15min', '60min']:\n",
    "    data_sets_multiindex[res_key + '_multiindex'] = data_sets[res_key]\n",
    "    \n",
    "    stacked = data_sets[res_key].copy()\n",
    "    stacked.columns = stacked.columns.droplevel(['source', 'web'])\n",
    "    stacked = stacked.transpose().stack(dropna=True).to_frame(name='data')\n",
    "    data_sets_stacked[res_key + '_stacked'] = stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Write to SQL-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file format is required for the filtering funtion on the OPSD website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for res_key, data_set in data_sets_singleindex.items():\n",
    "    f = 'timeseries' + res_key\n",
    "    ds = data_set.copy()\n",
    "    ds.index = ds.index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    ds.to_sql(f, sqlite3.connect(f + '.sqlite'),\n",
    "              if_exists='replace', index_label='timestamp') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Write to Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about 15 Minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for res_key, data_set in chain(data_sets_singleindex.items(),\n",
    "                               data_sets_multiindex.items()):\n",
    "    f = 'timeseries' + res_key\n",
    "    data_set.to_excel(f+ '.xlsx', float_format='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Write to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for res_key, data_set in chain(data_sets_singleindex.items(),\n",
    "                               data_sets_multiindex.items()):\n",
    "                               data_sets_stacked.items()):\n",
    "    f = 'timeseries' + res_key\n",
    "    data_set.to_csv(f + '.csv', float_format='%.2f',\n",
    "                    date_format='%Y-%m-%dT%H:%M:%SZ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Missing data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compact = data_sets['15min'].copy()\n",
    "compact.columns = compact.columns.droplevel([3, 4])\n",
    "compact.insert(0,'time',  compact.index.time)\n",
    "compact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for col in data_set.columns:\n",
    "#    df = col.to_frame()\n",
    "df = compact.iloc[:,15].to_frame()\n",
    "df['tag'] = ((df.index >= df.first_valid_index()) &\n",
    "             (df.index <= df.last_valid_index()) &\n",
    "             df.isnull().transpose().as_matrix()).transpose()\n",
    "\n",
    "# make another DF to hold info about each region\n",
    "regs_isnull = pd.DataFrame()\n",
    "\n",
    "# first row of consecutive region is a True preceded by a False in tags\n",
    "regs_isnull['start_idx']  = df.index[df['tag'] & ~ df['tag'].shift(1).fillna(False)]\n",
    "\n",
    "# last row of consecutive region is a False preceded by a True   \n",
    "regs_isnull['end_idx']  = df.index[df['tag'] & ~ df['tag'].shift(-1).fillna(False)] \n",
    "\n",
    "if df['tag'].any():\n",
    "    # how long is each region\n",
    "    regs_isnull['spans'] = regs_isnull['end_idx'] - regs_isnull['start_idx'] + timedelta(minutes=15)\n",
    "    \n",
    "    # index of the region with the longest span      \n",
    "    max_idx = regs_isnull['spans'].argmax()\n",
    "\n",
    "    # we can get the start and end points of longest region from the original dataframe \n",
    "    df.ix[regs_isnull.ix[max_idx][['start_idx', 'end_idx']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.ix[regs_isnull.ix[max_idx][['start_idx', 'end_idx']].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regs_isnull[regs_isnull['spans']>timedelta(minutes=60)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compact.insert(0,'time', compact.index.time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pv = compact.xs(('solar'), level=('variable'), axis=1, drop_level=False)\n",
    "pv.index = pd.MultiIndex.from_arrays([pv.index.date, pv.index.time], names=['date','time'])\n",
    "pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pv.groupby(level='time').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pv.unstack().idxmax().to_frame().unstack().transpose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
