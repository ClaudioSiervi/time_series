{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Power System Data: time series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the project [Open Power System Data](http://open-power-system-data.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "* [1. Introductory Notes](#1.-Introductory-Notes)\n",
    "* [2. Settings](#2.-Settings)\n",
    "* [3. Download](#3.-Download)\n",
    "* [4. Read](#4.-Read)\n",
    "* [5. Processing](#5.-Processing)\n",
    "\t* [5.1 Missing Data Handling](#5.1-Missing-Data-Handling)\n",
    "\t* [5.2 Aggregate German data from individual TSOs](#5.2-Aggregate-German-data-from-individual-TSOs)\n",
    "\t* [5.3 Create hourly data from 15' data](#5.3-Create-hourly-data-from-15'-data)\n",
    "* [6. Create metadata](#6.-Create-metadata)\n",
    "\t* [6.1 General metadata](#6.1-General-metadata)\n",
    "\t* [6.2 Columns-specific metadata](#6.2-Columns-specific-metadata)\n",
    "* [7. Write the data to disk](#7.-Write-the-data-to-disk)\n",
    "\t* [7.1 Write to SQL-database](#7.1-Write-to-SQL-database)\n",
    "\t* [7.2 Write to Excel](#7.2-Write-to-Excel)\n",
    "\t* [7.3 Write to CSV](#7.3-Write-to-CSV)\n",
    "* [8. Plausibility checks](#8.-Plausibility-checks)\n",
    "* [9. What is in the output files?](#9.-What-is-in-the-output-files?)\n",
    "\t* [9.1 Data sources](#9.1-Data-sources)\n",
    "\t* [9.2 Data documentation and interpretation](#9.2-Data-documentation-and-interpretation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introductory Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Jupyter notebook python script  downloads and processes time-series data from European power systems. The notebook has been used to create the [timeseries-datapackage](http://data.open-power-system-data.org/datapackage_timeseries/) that is available on the [Open Power System Data plattform](http://data.open-power-system-data.org/).\n",
    "\n",
    "A Jupyter notebook is a file that combines executable programming code with visualizations and comments in markdown format, allowing for an intuitive documentation of the code.\n",
    "\n",
    "The notebook is hosted in a [GitHub repository](https://github.com/Open-Power-System-Data/datapackage_timeseries) that can be [downloaded](https://github.com/Open-Power-System-Data/datapackage_timeseries/archive/master.zip) for execution on your local computer (You need a running python installation to do this, for example [Anaconda](https://www.continuum.io/downloads))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sources_yaml_path = 'config/sources.yml'\n",
    "out_path = 'original_data'\n",
    "\n",
    "# Optionally, specify a subset to download, e.g. subset=['TenneT', '50Hertz']\n",
    "#include_sources = ['Svenska_Kraftnaet']\n",
    "#include_sources = ['Amprion']\n",
    "#include_sources = None\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('log')\n",
    "logger.setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download sources are in `config/sources.yml`, which specifies, for each source, the variables (such as wind and solar generation) alongside all the parameters necessary to execute the downloads.\n",
    "\n",
    "First, a data directory is created on your local computer. Then, download parameters for each data source are defined, including the URL. These parameters are then turned into a YAML-string. Finally, the download is executed one by one. If all data need to be downloaded, this usually takes several hours.\n",
    "\n",
    "\n",
    "Each file is saved under it's original filename. Note that the original file names are often not self-explanatory (called \"data\" or \"January\"). The files content is revealed by its place in the directory structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from timeseries_scripts import download\n",
    "\n",
    "# Optionally, specify a subset to download, e.g. subset=['TenneT', '50Hertz']\n",
    "\n",
    "download.download(sources_yaml_path, out_path, subset=include_sources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reads each downloaded file into a pandas-DataFrame and merges data from different sources but with the same time resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the rows at the top of the data used to store metadata internally. In the output data, this information will be moved to the datapackage.json File."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HEADERS = ['variable', 'country', 'attribute', 'source', 'web']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from timeseries_scripts import read\n",
    "#import logging\n",
    "#import sys\n",
    "#logger = logging.getLogger()\n",
    "#logger.handlers[0].stream = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets = read.read(sources_yaml_path, out_path, HEADERS, subset=include_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_sets['15min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section performs some aggregations and transforms the data to the [tabular data package format](http://data.okfn.org/doc/tabular-data-package), where actual data is saved in a CSV file, while metadata (information on format, units, sources, and descriptions) is stored in a JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "#import pycountry\n",
    "import json\n",
    "import sqlite3\n",
    "import copy\n",
    "from itertools import chain\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Missing Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch missing data. At this stage, only implemented for 15 minute resolution solar/wind in-feed data from german TSOs. Small gaps (up to 2 hours) are filled by linear interpolation. For the generation timeseries, larger gaps are guessed by up-/down scaling the data from other balancing areas to fit the expected magnitude of the missing data.\n",
    "\n",
    "The locations of missing data are stored in the nan_table DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def patcher(frame):\n",
    "    '''Search for missing values in a DataFrame and apply custom patching.'''\n",
    "    nan_table = pd.DataFrame()\n",
    "    patched = pd.DataFrame()\n",
    "    one_period = frame.index[1] - frame.index[0]\n",
    "    for col_name, col in frame.iteritems():\n",
    "        df = col.to_frame()\n",
    "\n",
    "        # tag all occurences of NaN in the data (but not before first actual entry or after last one)\n",
    "        df['tag'] = ((df.index >= df.first_valid_index()) &\n",
    "                     (df.index <= df.last_valid_index()) &\n",
    "                     df.isnull().transpose().as_matrix()).transpose()\n",
    "\n",
    "        # make another DF to hold info about each region\n",
    "        nan_regs = pd.DataFrame()\n",
    "\n",
    "        # first row of consecutive region is a True preceded by a False in tags\n",
    "        nan_regs['start_idx'] = df.index[df['tag'] & ~ df['tag'].shift(1).fillna(False)]\n",
    "\n",
    "        # last row of consecutive region is a False preceded by a True   \n",
    "        nan_regs['till_idx'] = df.index[df['tag'] & ~ df['tag'].shift(-1).fillna(False)] \n",
    "\n",
    "        if not df['tag'].any():\n",
    "            logger.info('%s : nothing to patch in this column', col_name[0:3])\n",
    "            df.drop('tag', axis=1, inplace=True)\n",
    "            nan_idx = pd.MultiIndex.from_arrays([[0, 0, 0, 0], ['count', 'span', 'start_idx', 'till_idx']])\n",
    "            nan_list = pd.DataFrame(index=nan_idx, columns=df.columns)\n",
    "        else:\n",
    "            # how long is each region\n",
    "            nan_regs['span'] = nan_regs['till_idx'] - nan_regs['start_idx'] + one_period\n",
    "            nan_regs['count'] = (nan_regs['span'] / one_period)\n",
    "            # sort the info DF to put longest missing region on top\n",
    "            nan_regs = nan_regs.sort_values('count', ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            df.drop('tag', axis=1, inplace=True)\n",
    "            nan_list = nan_regs.stack().to_frame()\n",
    "            nan_list.columns = df.columns\n",
    "\n",
    "            for i, row in nan_regs.iterrows():\n",
    "                j = 0\n",
    "                # interpolate missing value spans up to 2 hours\n",
    "                if row['span'] <= timedelta(hours=2):\n",
    "                    if i + 1 == len(nan_regs):\n",
    "                        logger.info('%s : \\n        interpolated %s '\n",
    "                                    'up-to-2-hour-spans of NaNs',\n",
    "                                    col_name[0:3], i + 1 - j)\n",
    "                    to_fill = slice(row['start_idx'] - one_period,\n",
    "                                     row['till_idx'] + one_period)\n",
    "                    df.iloc[:,0].loc[to_fill] = df.iloc[:,0].loc[to_fill].interpolate()\n",
    "\n",
    "                # guess missing value spans longer than one hour based on other tsos\n",
    "                elif col_name[1][:2] == 'DE' and col_name[2] == 'generation':\n",
    "                    j += 1\n",
    "#                    logger.info('guessed %s entries after %s', row['count'], row['start_idx'])\n",
    "                    day_before = pd.DatetimeIndex(freq='15min',\n",
    "                                                  start=row['start_idx'] - timedelta(hours=24),\n",
    "                                                  end=row['start_idx'] - one_period)\n",
    "\n",
    "                    to_fill = pd.DatetimeIndex(freq='15min',\n",
    "                                                start=row['start_idx'],\n",
    "                                                end=row['till_idx'])\n",
    "\n",
    "                    # other_tsos = [c[1] for c in compact.drop(col_name, axis=1).loc[:,(col_name[0],slice(None),col_name[2])].columns.tolist()]\n",
    "                    other_tsos = [tso for tso in ['DE50hertz', 'DEamprion', 'DEtennet', 'DEtransnetbw'] if tso != col_name[1]]\n",
    "                    \n",
    "                    # select columns with data for same technology (wind/solar) but from other TSOs\n",
    "                    similar = frame.loc[:,(col_name[0],other_tsos,col_name[2])]\n",
    "                    # calculate the sum using columns without NaNs the day \n",
    "                    # before or during the period to be guessed\n",
    "                    similar = similar.dropna(axis=1, how='any', subset=day_before.append(to_fill)).sum(axis=1)\n",
    "                    # calculate scaling factor for other TSO data\n",
    "                    factor =  similar.loc[day_before].sum(axis=0) / df.loc[day_before,:].sum(axis=0)\n",
    "                    \n",
    "                    guess = similar.loc[to_fill] / float(factor)\n",
    "                    df.iloc[:,0].loc[to_fill] = guess\n",
    "                    a = float(df.iloc[:,0].loc[row['start_idx'] - one_period])\n",
    "                    b = float(df.iloc[:,0].loc[row['start_idx']])\n",
    "                    if a == 0:\n",
    "                        deviation = '{} absolut'.format(a - b)\n",
    "                    else:\n",
    "                        deviation = '{:.2f} %'.format((a - b) / a * 100)\n",
    "                    logger.info('%s : \\n        '\n",
    "                                'guessed %s entries after %s \\n        '\n",
    "                                'last non-missing: %s \\n        '\n",
    "                                'first guessed: %s \\n        '\n",
    "                                'deviation of first guess from last known value: %s', \n",
    "                                col_name[0:3], row['count'], row['start_idx'],\n",
    "                                a, b, deviation)                  \n",
    "\n",
    "        if len(nan_table) == 0:\n",
    "            nan_table = nan_list\n",
    "        else:\n",
    "            nan_table = nan_table.combine_first(nan_list)\n",
    "\n",
    "        if len(patched) == 0:\n",
    "            patched = df\n",
    "        else:\n",
    "            patched = patched.combine_first(df)\n",
    "            \n",
    "    nan_table.columns.names = HEADERS\n",
    "    patched.columns.names = HEADERS\n",
    "\n",
    "    return patched, nan_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch the 15 minutes dataset and display the location of missing Data in the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patched, nan_table = patcher(data_sets['15min'])\n",
    "nan_table#.loc[(slice(None),['count','start_idx']),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to see whether there is still missing data. This is the case for some of the forecast columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patched2, nan_table2 = patcher(patched)\n",
    "nan_table2#.loc[(slice(None),['count','start_idx']),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to see an example of where the data has been patched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min'].loc['2015-10-24 23:00:00':'2015-10-25 03:00:00', 'wind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patched.loc['2015-10-24 23:00:00':'2015-10-25 03:00:00', 'wind']b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the untreated data set with the patched one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min'] = patched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Aggregate German data from individual TSOs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wind and solar in-feed data for the 4 German balancing areas is summed up and stored in in new columns, which are then used to calculate profiles, that is, the share of wind/solar capacity producing at a given time. The column headers are created in the fashion introduced in the read script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "web = 'http://data.open-power-system-data.org/datapackage_timeseries'\n",
    "for tech in ['wind', 'solar']:\n",
    "    for attribute in ['generation', 'forecast']:\n",
    "        sum_col = pd.Series()\n",
    "        for tso in ['DE50hertz', 'DEamprion', 'DEtennet', 'DEtransnetbw']:\n",
    "            try:\n",
    "                add_col = data_sets['15min'][tech, tso, attribute]\n",
    "                if len(sum_col) == 0:\n",
    "                    sum_col = add_col\n",
    "                else:\n",
    "                    sum_col = sum_col + add_col.values\n",
    "            except KeyError:\n",
    "                pass\n",
    "                \n",
    "        # Create a new MultiIndex\n",
    "        tuples = [(tech, 'DE', attribute, 'own calculation', web)]\n",
    "        columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "        sum_col.columns = columns\n",
    "        data_sets['15min'] = data_sets['15min'].combine_first(sum_col)\n",
    "        \n",
    "        # Calculate the profile column\n",
    "        try:\n",
    "            if attribute == 'generation':\n",
    "                profile_col = sum_col.values / data_sets['15min'][tech, 'DE', 'capacity']\n",
    "                tuples = [(tech, 'DE', 'profile', 'own calculation', web)]\n",
    "                columns = pd.MultiIndex.from_tuples(tuples, names=HEADERS)\n",
    "                profile_col.columns = columns\n",
    "                data_sets['15min'] = data_sets['15min'].combine_first(profile_col)\n",
    "        except KeyError:\n",
    "            pass  # FIXME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New columns for the aggregated data have been added to the 15 minutes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['15min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Create hourly data from 15' data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The German renewables in-feed data comes in 15-minute intervals. We resample it to hourly intervals in order to match the load data from ENTSO-E."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "resampled = data_sets['15min'].resample('H').mean()\n",
    "try:\n",
    "    data_sets['60min'] = data_sets['60min'].combine_first(resampled)\n",
    "except KeyError:\n",
    "    data_sets['60min'] = resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New columns for the resampled data have been added to the 60 minutes dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets['60min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we create the metadata that will document the data output in CSV format. The metadata we be stored in JSON format, which is very much like a python dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 General metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we define the general metadata for the timeseries datapackage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    'name': 'opsd-timeseries',\n",
    "    'title': 'Time-series data: load, wind and solar, prices',\n",
    "    'description': 'This data package contains different kinds of timeseries '\n",
    "        'data relevant for power system modelling. Currently, the data '\n",
    "        'includes hourly electricity consumption (load) for 36 European '\n",
    "        'countries, wind and solar power generation from German transmission '\n",
    "        'system operators for every quarter hour, and daily wind and solar '\n",
    "        'capacity data. We use '\n",
    "        'this data to calculate Germany-wide renewables in-feed and profile '\n",
    "        'timeseries. We plan to include more data sources in the future. '\n",
    "        'While some of the wind in-feed data dates back to '\n",
    "        '2005, the full dataset is only available from 2012 onwards. The '\n",
    "        'data has been downloaded from the sources, resampled and merged in '\n",
    "        'a large CSV file with hourly resolution. Additionally, the data '\n",
    "        'available at a higher resolution (German renewables in-feed, 15 '\n",
    "        'minutes) is provided in a separate file. All data processing is '\n",
    "        'conducted in python and pandas and has been documented in the '\n",
    "        'Jupyter notebooks linked below.',\n",
    "    'opsd-jupyter-notebook-url': 'https://github.com/Open-Power-System-Data/'\n",
    "        'datapackage_timeseries/blob/master/main.ipynb',\n",
    "    'version': '2016-03-30',\n",
    "    'opsd-changes-to-last-version': 'Added missing data handling to patch '\n",
    "        'gaps in the data from German TSOs',\n",
    "    'keywords': [\n",
    "        'timeseries','electricity','in-feed','capacity','renewables', 'wind',\n",
    "        'solar','load','tso','europe','germany'\n",
    "        ],\n",
    "    'geographical-scope': 'Europe/Germany',\n",
    "    'licenses': [{\n",
    "        'url': 'http://example.com/license/url/here',\n",
    "        'version': '1.0',\n",
    "        'name': 'License Name Here',\n",
    "        'id': 'license-id-from-open'\n",
    "        }],\n",
    "    'views': [{}],\n",
    "    'sources': [{\n",
    "        'name': 'See the \"Source\" column in the field documentation'\n",
    "        }],\n",
    "    'maintainers': [{\n",
    "        'web': 'http://example.com/',\n",
    "        'name': 'Jonathan Muehlenpfordt',\n",
    "        'email': 'muehlenpfordt@neon-energie.de'\n",
    "        }],\n",
    "    'resources': [{ # The following is an example of how the file-specific metadata is \n",
    "        'path': 'path_to.csv', # structured. The actual metadata is created below\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {\n",
    "            'fields': [{\n",
    "                'name': 'load_AT_actual',\n",
    "                'description': 'Consumption in Austria in MW',\n",
    "                'type': 'number',\n",
    "                'source': {\n",
    "                    'name': 'Example',\n",
    "                    'web': 'http://www.example.com'\n",
    "                    },\n",
    "                'opsd-properties': {\n",
    "                    'Country': 'AT',\n",
    "                    'Variable': 'load',\n",
    "                    }\n",
    "                }]\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    "\n",
    "indexfield = {\n",
    "    'name': 'timestamp',\n",
    "    'description': 'Start of timeperiod in UTC',\n",
    "    'type': 'datetime',\n",
    "    'format': 'YYYY-MM-DDThh:mm:ssZ'\n",
    "    }\n",
    "\n",
    "descriptions = {\n",
    "    'load': 'Consumption in {geo} in MW',\n",
    "    'generation': 'Actual {tech} generation in {geo} in MW',\n",
    "    'forecast': '{tech} day-ahead generation forecast in {geo} in MW',\n",
    "    'capacity': '{tech} capacity in {geo} in MW',\n",
    "    'profile': 'Share of {tech} capacity producing in {geo}',\n",
    "    'offshoreshare': '{tech} actual offshore generation in {geo} in MW'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Columns-specific metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each dataset/outputfile, the metadata has an entry in the \"resources\" list that describes the file/dataset. The main part of each entry is the \"schema\" dictionary, consisting of a list of \"fields\", meaning the columns in the dataset. The first field is the timestamp index of the dataset. For the other fields, we iterate over the columns of the MultiIndex index of the datasets to contruct the corresponding metadata.\n",
    "\n",
    "At the same time, a copy of the datasets is created that has a single line column index instead of the MultiIndex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets_singleindex = copy.deepcopy(data_sets)##########################\n",
    "resources = []\n",
    "for res_key, data_set in data_sets.items():\n",
    "    columns_singleindex = [] ####################\n",
    "    fields = [indexfield]\n",
    "    for col in data_set.columns:\n",
    "        h = {k: v for k, v in zip(HEADERS, col)}\n",
    "        if len(h['country']) > 2:\n",
    "            geo = h['country'] + ' control area'\n",
    "        elif h['country'] == 'NI':\n",
    "            geo = 'Northern Ireland'\n",
    "        elif h['country'] == 'CS':\n",
    "            geo = 'Serbia and Montenegro'\n",
    "        else:\n",
    "            geo = pycountry.countries.get(alpha2=h['country']).name\n",
    "\n",
    "        field = {}    \n",
    "        field['description'] = descriptions[h['attribute']].format(\n",
    "            tech=h['variable'], geo=geo)\n",
    "        field['type'] = 'number'\n",
    "        field['source'] = {\n",
    "            'name': h['source'],\n",
    "            'web': h['web']\n",
    "            }\n",
    "        field['opsd-properties'] = {\n",
    "            'Country': h['country'],\n",
    "            'Variable': h['variable'],\n",
    "            }\n",
    "        components = [h['variable'], h['country']]\n",
    "        if not h['variable'] == 'load':\n",
    "            components.append(h['attribute'])\n",
    "            field['opsd-properties']['Attribute'] = h['attribute']\n",
    "        field['name'] = '_'.join(components)\n",
    "        columns_singleindex.append(field['name'])\n",
    "        fields.append(field)\n",
    "        \n",
    "    resource = {\n",
    "        'path': 'timeseries' + res_key + '.csv',\n",
    "        'format': 'csv',\n",
    "        'mediatype': 'text/csv',\n",
    "        'schema': {'fields': fields},\n",
    "        'alternative_formats': [\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '.csv',\n",
    "          'stacking': 'Singleindex',\n",
    "          'format': 'csv'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '.xlsx',\n",
    "          'stacking': 'Singleindex',\n",
    "          'format': 'xlsx'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '_multiindex.xlsx',\n",
    "          'stacking': 'Multiindex',\n",
    "          'format': 'xlsx'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '_multiindex.csv',\n",
    "          'stacking': 'Multiindex',\n",
    "          'format': 'csv'\n",
    "        },\n",
    "        {\n",
    "          'path': 'timeseries' + res_key + '_stacked.csv',\n",
    "          'stacking': 'Stacked',\n",
    "          'format': 'csv'\n",
    "        }\n",
    "        ]\n",
    "    }       \n",
    "    resources.append(resource)\n",
    "    data_sets_singleindex[res_key].columns = columns_singleindex ###############\n",
    "    \n",
    "metadata['resources'] = resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute this to write the metadata to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datapackage_json = json.dumps(metadata, indent=2, separators=(',', ': '))\n",
    "with open('datapackage.json', 'w') as f:\n",
    "    f.write(datapackage_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Write the data to disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want to write the data to the output files and save it in the directory of this notebook. First, we prepare different shapes of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets_multiindex = {}\n",
    "data_sets_stacked = {}\n",
    "for res_key in ['15min', '60min']:\n",
    "    data_sets_multiindex[res_key + '_multiindex'] = data_sets[res_key]\n",
    "    \n",
    "    stacked = data_sets[res_key].copy()\n",
    "    stacked.columns = stacked.columns.droplevel(['source', 'web'])\n",
    "    stacked = stacked.transpose().stack(dropna=True).to_frame(name='data')\n",
    "    data_sets_stacked[res_key + '_stacked'] = stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Write to SQL-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file format is required for the filtering function on the OPSD website. This takes about 30 seconds to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_sql(path):\n",
    "    for res_key, data_set in data_sets_singleindex.items():\n",
    "        table = 'timeseries' + res_key\n",
    "        ds = data_set.copy()\n",
    "        ds.index = ds.index.strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "        ds.to_sql(table, sqlite3.connect(path),\n",
    "                  if_exists='replace', index_label='timestamp')\n",
    "    return\n",
    "\n",
    "write_sql('data.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Write to Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about 15 Minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_excel():\n",
    "    for res_key, data_set in chain(data_sets_singleindex.items(),\n",
    "                                   data_sets_multiindex.items()):\n",
    "        f = 'timeseries' + res_key\n",
    "        data_set.to_excel(f+ '.xlsx', float_format='%.2f')\n",
    "write_excel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Write to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes about 10 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_csv():\n",
    "    for res_key, data_set in chain(data_sets_singleindex.items(),\n",
    "                                   data_sets_multiindex.items(),\n",
    "                                   data_sets_stacked.items()):\n",
    "        f = 'timeseries' + res_key\n",
    "        data_set.to_csv(f + '.csv', float_format='%.2f',\n",
    "                        date_format='%Y-%m-%dT%H:%M:%SZ')\n",
    "write_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Plausibility checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "work in progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pv = compact.xs(('solar'), level=('variable'), axis=1, drop_level=False)\n",
    "# pv.index = pd.MultiIndex.from_arrays([pv.index.date, pv.index.time], names=['date','time'])\n",
    "# pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pv.groupby(level='time').max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pv.unstack().idxmax().to_frame().unstack().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. What is in the output files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Power System Data: time series\n",
    "Part of the project [Open Power System Data](http://open-power-system-data.org/).\n",
    "\n",
    "This is the first of 4 Jupyter notebook python scripts that downloads and processes time-series data from European power systems. The notebooks have been used to create the [timeseries-datapackage](http://data.open-power-system-data.org/datapackage_timeseries/) that is available on the [Open Power System Data plattform](http://data.open-power-system-data.org/). A Jupyter notebook is a file that combines executable programming code with visualizations and comments in markdown format, allowing for an intuitive documentation of the code.\n",
    "\n",
    "The notebooks are part of a [GitHub repository](https://github.com/Open-Power-System-Data/datapackage_timeseries) and can be [downloaded](https://github.com/Open-Power-System-Data/datapackage_timeseries/archive/master.zip) for execution on your local computer (You need a running python installation to do this, for example [Anaconda](https://www.continuum.io/downloads)).  Executed one after another, they can be used to reproduce the dataset that we provide for download."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 Data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An overview of the sources for the data can be found [here](http://open-power-system-data.org/opsd-sources#time-series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.2 Data documentation and interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often, the data that we use is poorly documented. In some cases, primary data owners provide some documentation.\n",
    "\n",
    "\n",
    "**Load data**\n",
    "* [ENTSO-E Specific national considerations](https://www.entsoe.eu/Documents/Publications/Statistics/Specific_national_considerations.pdf)\n",
    "* [Schumacher & Hirth 2015](http://papers.ssrn.com/sol3/papers.cfm?abstract_id=2715986), a paper on load data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
